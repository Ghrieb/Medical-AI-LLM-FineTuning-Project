# Medical-AI-LLM-FineTuning-Project
This project showcases the fine-tuning of a large language model (Meta Llama 2) to become an expert in the medical domain, specifically focusing on genomic characterization and viral genomics. It was developed as part of the "Introducing Generative AI with AWS" course offered by Udacity and AWS.

The aim of this project is to demonstrate the power of Generative AI and Machine Learning to solve real-world challenges in biomedical research and healthcare, highlighting how AI models can be fine-tuned to generate contextually accurate and domain-specific text, making it a valuable asset in fields such as medical diagnostics, personalized medicine, and healthcare innovation.

# Project Summary
As a dual major student in Developmental Biology and Computer Science, this project uniquely bridges my knowledge in both fields. The goal was to transform a pre-trained LLM into a domain-specific medical model through fine-tuning, aligning the project with the medical field's need for AI-powered solutions in areas such as genomics, viral research, and personalized medicine.

The project was developed using AWS SageMaker, and with a strict $25 budget allocation, the model was successfully deployed and fine-tuned, emphasizing resource efficiency and scalability.

# Course Information: "Introducing Generative AI with AWS"
This course, offered by Udacity in partnership with AWS, is designed to provide hands-on experience in Generative AI using AWS SageMaker and state-of-the-art models like Meta Llama 2. The course covers the following areas:

- AI and ML foundations: Introduction to Generative AI and how large language models (LLMs) work.
- Model Deployment: How to deploy and use pre-trained models using AWS SageMaker.
- Fine-Tuning Large Models: Training models on domain-specific datasets.
- Text Generation: Using AI to generate high-quality, domain-specific text content.

# Project Objectives
## 1. Deploy Meta Llama 2 on AWS SageMaker:
- Use Meta Llama 2 7B as the base model, which was pre-trained for text generation tasks.
- Evaluate the pre-trained model to assess its general ability to understand medical content.

## 2. Fine-Tune the Model for Medical Domain:
- Fine-tune the pre-trained model using a medical dataset focused on genomic characterization and viral genomics.
- Adapt the model to produce more contextually accurate and domain-specific responses related to genomic research and biomedical sciences.

## 3. Evaluate the Fine-Tuned Model:
- Test the fine-tuned model to compare its performance with the pre-trained version, specifically on medical text generation tasks.
- Analyze improvements in the modelâ€™s ability to understand and generate accurate medical terminology and research insights.
